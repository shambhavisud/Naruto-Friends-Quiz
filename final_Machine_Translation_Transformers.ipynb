{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "final_Machine_Translation_Transformers.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shambhavisud/Naruto-Friends-Quiz/blob/master/final_Machine_Translation_Transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUqYrUbpJh5t"
      },
      "source": [
        "## Necessary Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eD6jlbDHz8W-"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import defaultdict\n",
        "import string\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Model\n",
        "import tensorflow as tf\n",
        "import re\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "import os\n",
        "import time\n",
        "import seaborn as sns\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rqj3nxrr0Ewo"
      },
      "source": [
        "\n",
        "n_size_batch = 64\n",
        "n_size_buffer = n_size_batch*4\n",
        "len_dcdr = 100\n",
        "len_enc = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l85dXfhE0BYh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "995082b5-6505-4ecb-ce82-b9d499e5373e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqU9zsYdJ3Hc"
      },
      "source": [
        "#Data Preprocessing\n",
        "<h2>Loading Files</h2>\n",
        "To improve performance of the model, I filtered and deleted phrases that either alot of characters or only a few of them. Both Hindi & English sentences were then tokenized & padded to the fixed length.\n",
        "\n",
        "[link to data](https://www.kaggle.com/aiswaryaramachandran/hindienglish-corpora)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvsSZJxUTkiu"
      },
      "source": [
        "to_df_trn = pd.read_csv(\"/content/drive/My Drive/translation_files/Hindi_English_Truncated_Corpus.csv\")\n",
        "to_df_trn.drop(['source'],axis=1,inplace=True)\n",
        "mask = (to_df_trn['english_sentence'].str.len()>20) & (to_df_trn['english_sentence'].str.len()<200)\n",
        "to_df_trn = to_df_trn.loc[mask]\n",
        "to_df_trn = to_df_trn.sample(64000, random_state=1)\n",
        "to_df_trn.head()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjotmONeT5Ly"
      },
      "source": [
        "pd.set_option('display.max_colwidth', None)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8147W6NeUR-7"
      },
      "source": [
        "English = to_df_trn['english_sentence']\n",
        "Hindi = to_df_trn['hindi_sentence']\n",
        "English = English.apply(lambda x: \"<SOS> \" + str(x) + \" <EOS>\")\n",
        "Hindi = Hindi.apply(lambda x: \"<SOS> \"+ x + \" <EOS>\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizing Text"
      ],
      "metadata": {
        "id": "ecxYsZyFmiAZ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mRPR8tPUUcS"
      },
      "source": [
        "filters = '!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n'\n",
        "oov_token = '<unk>'\n",
        "tkniser_en = tf.keras.preprocessing.text.Tokenizer(filters = filters, oov_token=oov_token)\n",
        "tkniser_hi = tf.keras.preprocessing.text.Tokenizer(filters = filters, oov_token=oov_token)\n",
        "tkniser_en.fit_on_texts(English)\n",
        "tkniser_hi.fit_on_texts(Hindi)\n",
        "ips = tkniser_en.texts_to_sequences(English)\n",
        "i_trgts = tkniser_hi.texts_to_sequences(Hindi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsDChPeuUXsQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7436d887-ceda-4fdd-b04a-c774e8e60058"
      },
      "source": [
        "vocab_encdr = len(tkniser_en.word_index) + 1\n",
        "vocab_dcdr = len(tkniser_hi.word_index) + 1\n",
        "print(vocab_encdr, vocab_dcdr)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "44305 51960\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inputs/outputs creation\n",
        "As we train with batches, we need each input to have the same length. We pad with the appropriate token, and we will make sure this padding token doesn't interfere with our training later."
      ],
      "metadata": {
        "id": "D3EA-1_wm9f2"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QuVBpEOCUmDl"
      },
      "source": [
        "ips = tf.keras.preprocessing.sequence.pad_sequences(ips, maxlen=len_enc, padding='post', truncating='post')\n",
        "i_trgts = tf.keras.preprocessing.sequence.pad_sequences(i_trgts, maxlen=len_dcdr, padding='post', truncating='post')\n",
        "ips = tf.cast(ips, dtype=tf.int64)\n",
        "i_trgts = tf.cast(i_trgts, dtype=tf.int64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_ZGHWXwUrxY"
      },
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices((ips, i_trgts)).shuffle(n_size_buffer).batch(n_size_batch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwEac4hQLZeD"
      },
      "source": [
        "# Transformer Model Building\n",
        "Here I build a Transformer from scratch.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embedding\n"
      ],
      "metadata": {
        "id": "A7AWZ2JgyAyk"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2OsiG6ZPUt97"
      },
      "source": [
        "def fetch_angles(position, i, d_model):\n",
        "    rts_angle = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
        "    return position * rts_angle\n",
        "\n",
        "def encd_position(position, d_model):\n",
        "    rds_angl = fetch_angles(\n",
        "        np.arange(position)[:, np.newaxis],\n",
        "        np.arange(d_model)[np.newaxis, :],\n",
        "        d_model\n",
        "    )\n",
        "\n",
        "    rds_angl[:, 0::2] = np.sin(rds_angl[:, 0::2])\n",
        "\n",
        "    rds_angl[:, 1::2] = np.cos(rds_angl[:, 1::2])\n",
        "\n",
        "    encd_pos = rds_angl[np.newaxis, ...]\n",
        "\n",
        "    return tf.cast(encd_pos, dtype=tf.float32)\n",
        "\n",
        "def make_msk_padding(seq):\n",
        "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "def make_msk_look_ahead(size):\n",
        "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "    return mask\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Attention computation"
      ],
      "metadata": {
        "id": "sCB__hIZyR_t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$Attention(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$\n",
        "\n",
        "Q, K and V are matrices representing sequences/sentences after embedding. Q is my context sequence B. we divide by dk which is the dimension of each element from each word to the key, it makes the model more stable.</br>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jMjFvLwCyVHB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def attn_dot_prod_scld(q, k, v, mask):\n",
        "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
        "\n",
        "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "    attn_logits_scld = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "    if mask is not None:\n",
        "        attn_logits_scld += (mask * -1e9)  \n",
        "\n",
        "    wts_attn = tf.nn.softmax(attn_logits_scld, axis=-1)\n",
        "\n",
        "    output = tf.matmul(wts_attn, v)\n",
        "    return output, wts_attn"
      ],
      "metadata": {
        "id": "B1uMblsnyQ0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-head attention sublayer"
      ],
      "metadata": {
        "id": "4pnZMNf1yeDm"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKfzlTmsWJTO"
      },
      "source": [
        "class attn_mlti_hds(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(attn_mlti_hds, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "\n",
        "        assert d_model % self.num_heads == 0\n",
        "\n",
        "        self.depth = d_model // self.num_heads\n",
        "\n",
        "        self.wq = tf.keras.layers.Dense(d_model)\n",
        "        self.wk = tf.keras.layers.Dense(d_model)\n",
        "        self.wv = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "        self.dense = tf.keras.layers.Dense(d_model)\n",
        "        \n",
        "    def hd_splt(self, x, batch_size):\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "    \n",
        "    def call(self, v, k, q, mask):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "\n",
        "        q = self.wq(q)\n",
        "        k = self.wk(k)\n",
        "        v = self.wv(v)\n",
        "\n",
        "        q = self.hd_splt(q, batch_size)\n",
        "        k = self.hd_splt(k, batch_size)\n",
        "        v = self.hd_splt(v, batch_size)\n",
        "\n",
        "        attn_scld, wts_attn = attn_dot_prod_scld(\n",
        "            q, k, v, mask)\n",
        "\n",
        "        attn_scld = tf.transpose(attn_scld, perm=[0, 2, 1, 3])\n",
        "        \n",
        "        attn_concat = tf.reshape(attn_scld, (batch_size, -1, self.d_model))\n",
        "        output = self.dense(attn_concat)\n",
        "            \n",
        "        return output, wts_attn\n",
        "    \n",
        "def ffnn_pt_wise(d_model, dff):\n",
        "    return tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(dff, activation='relu'),\n",
        "        tf.keras.layers.Dense(d_model)\n",
        "    ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoder\n",
        "Positional encoding formulae:\n",
        "\n",
        "$PE_{(pos, 2i)} = \\sin(pos/10000^{2i/dmodel})$\n",
        "\n",
        "$PE_{(pos, 2i+1)} = \\cos(pos/10000^{2i/dmodel})$"
      ],
      "metadata": {
        "id": "C85jpePHykXS"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Q5d7Lw9WNlg"
      },
      "source": [
        "class lr_encdr(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(lr_encdr, self).__init__()\n",
        "\n",
        "        self.mha = attn_mlti_hds(d_model, num_heads)\n",
        "        self.ffn = ffnn_pt_wise(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "    def call(self, x, training, mask):\n",
        "        attn_output, _ = self.mha(x, x, x, mask)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(x + attn_output)\n",
        "\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        out2 = self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "        return out2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decoder"
      ],
      "metadata": {
        "id": "WashY2V7yprs"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHsuR6e2WStT"
      },
      "source": [
        "class lr_dcdr(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(lr_dcdr, self).__init__()\n",
        "\n",
        "        self.mha1 = attn_mlti_hds(d_model, num_heads)\n",
        "        self.mha2 = attn_mlti_hds(d_model, num_heads)\n",
        "\n",
        "        self.ffn = ffnn_pt_wise(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "    \n",
        "    def call(self, x, op_enc, training, look_ahead_mask, padding_mask):\n",
        "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)\n",
        "        attn1 = self.dropout1(attn1, training=training)\n",
        "        out1 = self.layernorm1(attn1 + x)\n",
        "\n",
        "        attn2, attn_weights_block2 = self.mha2(op_enc, op_enc, out1, padding_mask)\n",
        "        attn2 = self.dropout2(attn2, training=training)\n",
        "        out2 = self.layernorm2(attn2 + out1)\n",
        "\n",
        "        ffn_output = self.ffn(out2)\n",
        "        ffn_output = self.dropout3(ffn_output, training=training)\n",
        "        out3 = self.layernorm3(ffn_output + out2)\n",
        "\n",
        "        return out3, attn_weights_block1, attn_weights_block2\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVjeLAuDWXXV"
      },
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, encd_position_max, rate=0.1):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
        "        self.encd_pos = encd_position(encd_position_max, self.d_model)\n",
        "\n",
        "        self.enc_layers = [lr_encdr(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
        "\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "        \n",
        "    def call(self, x, training, mask):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "\n",
        "        x = self.embedding(x)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.encd_pos[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "    \n",
        "        for i in range(self.num_layers):\n",
        "            x = self.enc_layers[i](x, training, mask)\n",
        "    \n",
        "        return x\n",
        "    \n",
        "class Decoder(tf.keras.layers.Layer):\n",
        "        \n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, encd_position_max, rate=0.1):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
        "        self.encd_pos = encd_position(encd_position_max, d_model)\n",
        "\n",
        "        self.dec_layers = [lr_dcdr(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "    def call(self, x, op_enc, training, look_ahead_mask, padding_mask):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        wts_attn = {}\n",
        "\n",
        "        x = self.embedding(x)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.encd_pos[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x, block1, block2 = self.dec_layers[i](x, op_enc, training, look_ahead_mask, padding_mask)\n",
        "\n",
        "            wts_attn['decoder_layer{}_block1'.format(i+1)] = block1\n",
        "            wts_attn['decoder_layer{}_block2'.format(i+1)] = block2\n",
        "    \n",
        "        return x, wts_attn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer\n",
        "[link](https://www.tensorflow.org/text/tutorials/transformer) to tutorial i used for building the Transformer from scratch"
      ],
      "metadata": {
        "id": "k8AXILcAyu8n"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KlMi9zUGWyrl"
      },
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input, pe_target, rate=0.1):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, pe_input, rate)\n",
        "\n",
        "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, rate)\n",
        "\n",
        "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "    \n",
        "    def call(self, inp, tar, training, msk_encdr_padding, look_ahead_mask, msk_dcdr_padding):\n",
        "        op_enc = self.encoder(inp, training, msk_encdr_padding)\n",
        "\n",
        "        op_dcdr, wts_attn = self.decoder(tar, op_enc, training, look_ahead_mask, msk_dcdr_padding)\n",
        "\n",
        "        op_resultant = self.final_layer(op_dcdr)\n",
        "\n",
        "        return op_resultant, wts_attn\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Application"
      ],
      "metadata": {
        "id": "tJRkNGyky8n9"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpXDJuVTW2n-"
      },
      "source": [
        "nepochs = 20\n",
        "#nepochs = 10\n",
        "d_model = 128\n",
        "num_layers = 4\n",
        "#nepochs = 10\n",
        "num_heads = 8\n",
        "dff = 512\n",
        "#nepochs = 10\n",
        "dropout_rate = 0.1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZoRe76S6LtRB"
      },
      "source": [
        "# Custom Learning Rate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKdLhN4DW452"
      },
      "source": [
        "class sched_custm(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(sched_custm, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "        self.warmup_steps = warmup_steps\n",
        "    \n",
        "    def __call__(self, step):\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYrWPX4PW-YL"
      },
      "source": [
        "rt_of_lrning = sched_custm(d_model)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(rt_of_lrning, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "Q3zSAnFNXBKT",
        "outputId": "aa24ad09-76c7-4a60-8c48-c11b4f6f5876"
      },
      "source": [
        "sched_tmp_lrng_rate = sched_custm(d_model)\n",
        "\n",
        "plt.plot(sched_tmp_lrng_rate(tf.range(40000, dtype=tf.float32)))\n",
        "plt.ylabel(\"Rate of Learning\")\n",
        "plt.xlabel(\"Train Step\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'Train Step')"
            ]
          },
          "metadata": {},
          "execution_count": 21
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7ogsKZzLzSu"
      },
      "source": [
        "# Custom Accuracy & Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPHWl5TkXDHa"
      },
      "source": [
        "obj_loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "def func_loss(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = obj_loss(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
        "\n",
        "\n",
        "def func_acc(real, pred):\n",
        "    accuracies = tf.equal(real, tf.argmax(pred, axis=2))\n",
        "\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    accuracies = tf.math.logical_and(mask, accuracies)\n",
        "\n",
        "    accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
        "    mask = tf.cast(mask, dtype=tf.float32)\n",
        "    return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itYFErPBXL_z"
      },
      "source": [
        "loss_tr = tf.keras.metrics.Mean(name='loss_tr')\n",
        "acc_tr = tf.keras.metrics.Mean(name='acc_tr')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j43BHxdgXORz"
      },
      "source": [
        "transformer = Transformer(\n",
        "    num_layers=num_layers,\n",
        "    d_model=d_model,\n",
        "    num_heads=num_heads,\n",
        "    dff=dff,\n",
        "    input_vocab_size=vocab_encdr,\n",
        "    target_vocab_size=vocab_dcdr,\n",
        "    pe_input=1000,\n",
        "    pe_target=1000,\n",
        "    rate=dropout_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtthUmj-XQPb"
      },
      "source": [
        "def make_msk(inp, tar):\n",
        "    msk_encdr_padding = make_msk_padding(inp)\n",
        "    msk_dcdr_padding = make_msk_padding(inp)\n",
        "\n",
        "    look_ahead_mask = make_msk_look_ahead(tf.shape(tar)[1])\n",
        "    dec_target_padding_mask = make_msk_padding(tar)\n",
        "    msk_comb = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "  \n",
        "    return msk_encdr_padding, msk_comb, msk_dcdr_padding"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmlV3HmMXSTi"
      },
      "source": [
        "dir_chkpt = \"checkpoints\"\n",
        "\n",
        "chkpt = tf.train.Checkpoint(transformer=transformer, optimizer=optimizer)\n",
        "\n",
        "mngr_chkpt = tf.train.CheckpointManager(chkpt, dir_chkpt, max_to_keep=5)\n",
        "\n",
        "if mngr_chkpt.latest_checkpoint:\n",
        "    chkpt.restore(mngr_chkpt.latest_checkpoint)\n",
        "    print ('updated chkpoint test')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4avOaVCzXasA"
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, tar):\n",
        "    tar_inp = tar[:, :-1]\n",
        "    tar_real = tar[:, 1:]\n",
        "\n",
        "    msk_encdr_padding, msk_comb, msk_dcdr_padding = make_msk(inp, tar_inp)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions, _ = transformer(\n",
        "            inp, tar_inp, \n",
        "            True, \n",
        "            msk_encdr_padding, \n",
        "            msk_comb, \n",
        "            msk_dcdr_padding\n",
        "        )\n",
        "        loss = func_loss(tar_real, predictions)\n",
        "\n",
        "    gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
        "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "\n",
        "    loss_tr(loss)\n",
        "    acc_tr(func_acc(tar_real, predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6VsKASzL6jX"
      },
      "source": [
        "#Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4eAM2Y_XgAX",
        "outputId": "ff8de57b-f2bc-4071-8bae-e1c955e7258e"
      },
      "source": [
        "for epoch in range(nepochs):\n",
        "    time_init = time.time()\n",
        "\n",
        "    loss_tr.reset_states()\n",
        "  \n",
        "    for (batch, (inp, tar)) in enumerate(dataset):\n",
        "        train_step(inp, tar)\n",
        "    \n",
        "        if batch % 200 == 0:\n",
        "            print(f'Epoch {epoch + 1} acc {acc_tr.result():.4f} loss {loss_tr.result():.4f} batch_no. {batch}  ')\n",
        "      \n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        dir_chkpt_save = mngr_chkpt.save()\n",
        "        print ('Saving checkpoint for epoch {} at {}'.format(epoch+1, dir_chkpt_save))\n",
        "   \n",
        "    print(f'Epoch {epoch + 1} acc {acc_tr.result():.4f} loss {loss_tr.result():.4f}')\n",
        "    print ('Single epoch took : {} seconds\\n'.format(time.time() - time_init))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 acc 0.0000 loss 10.8629 batch_no. 0  \n",
            "Epoch 1 acc 0.0427 loss 10.5080 batch_no. 200  \n",
            "Epoch 1 acc 0.0517 loss 9.6748 batch_no. 400  \n",
            "Epoch 1 acc 0.0560 loss 8.9393 batch_no. 600  \n",
            "Epoch 1 acc 0.0644 loss 8.4868 batch_no. 800  \n",
            "Epoch 1 acc 0.0736 loss 8.1596\n",
            "Single epoch took : 565.278332233429 seconds\n",
            "\n",
            "Epoch 2 acc 0.0737 loss 6.8752 batch_no. 0  \n",
            "Epoch 2 acc 0.0823 loss 6.6362 batch_no. 200  \n",
            "Epoch 2 acc 0.0900 loss 6.5562 batch_no. 400  \n",
            "Epoch 2 acc 0.0972 loss 6.4826 batch_no. 600  \n",
            "Epoch 2 acc 0.1036 loss 6.4118 batch_no. 800  \n",
            "Epoch 2 acc 0.1095 loss 6.3461\n",
            "Single epoch took : 553.7737319469452 seconds\n",
            "\n",
            "Epoch 3 acc 0.1095 loss 5.8160 batch_no. 0  \n",
            "Epoch 3 acc 0.1150 loss 5.9772 batch_no. 200  \n",
            "Epoch 3 acc 0.1201 loss 5.9220 batch_no. 400  \n",
            "Epoch 3 acc 0.1251 loss 5.8709 batch_no. 600  \n",
            "Epoch 3 acc 0.1298 loss 5.8211 batch_no. 800  \n",
            "Epoch 3 acc 0.1342 loss 5.7760\n",
            "Single epoch took : 554.051075220108 seconds\n",
            "\n",
            "Epoch 4 acc 0.1342 loss 5.4537 batch_no. 0  \n",
            "Epoch 4 acc 0.1384 loss 5.5000 batch_no. 200  \n",
            "Epoch 4 acc 0.1424 loss 5.4548 batch_no. 400  \n",
            "Epoch 4 acc 0.1464 loss 5.4097 batch_no. 600  \n",
            "Epoch 4 acc 0.1502 loss 5.3711 batch_no. 800  \n",
            "Epoch 4 acc 0.1539 loss 5.3332\n",
            "Single epoch took : 554.4160912036896 seconds\n",
            "\n",
            "Epoch 5 acc 0.1539 loss 5.2058 batch_no. 0  \n",
            "Epoch 5 acc 0.1576 loss 5.0972 batch_no. 200  \n",
            "Epoch 5 acc 0.1612 loss 5.0513 batch_no. 400  \n",
            "Epoch 5 acc 0.1650 loss 4.9997 batch_no. 600  \n",
            "Epoch 5 acc 0.1687 loss 4.9558 batch_no. 800  \n",
            "Saving checkpoint for epoch 5 at checkpoints/ckpt-1\n",
            "Epoch 5 acc 0.1724 loss 4.9126\n",
            "Single epoch took : 555.3174965381622 seconds\n",
            "\n",
            "Epoch 6 acc 0.1724 loss 4.8202 batch_no. 0  \n",
            "Epoch 6 acc 0.1761 loss 4.6595 batch_no. 200  \n",
            "Epoch 6 acc 0.1797 loss 4.6250 batch_no. 400  \n",
            "Epoch 6 acc 0.1834 loss 4.5838 batch_no. 600  \n",
            "Epoch 6 acc 0.1870 loss 4.5521 batch_no. 800  \n",
            "Epoch 6 acc 0.1906 loss 4.5200\n",
            "Single epoch took : 554.2929131984711 seconds\n",
            "\n",
            "Epoch 7 acc 0.1906 loss 4.2291 batch_no. 0  \n",
            "Epoch 7 acc 0.1943 loss 4.3320 batch_no. 200  \n",
            "Epoch 7 acc 0.1978 loss 4.3018 batch_no. 400  \n",
            "Epoch 7 acc 0.2014 loss 4.2694 batch_no. 600  \n",
            "Epoch 7 acc 0.2050 loss 4.2426 batch_no. 800  \n",
            "Epoch 7 acc 0.2084 loss 4.2175\n",
            "Single epoch took : 554.2845838069916 seconds\n",
            "\n",
            "Epoch 8 acc 0.2085 loss 4.1826 batch_no. 0  \n",
            "Epoch 8 acc 0.2119 loss 4.0769 batch_no. 200  \n",
            "Epoch 8 acc 0.2153 loss 4.0488 batch_no. 400  \n",
            "Epoch 8 acc 0.2187 loss 4.0194 batch_no. 600  \n",
            "Epoch 8 acc 0.2221 loss 3.9990 batch_no. 800  \n",
            "Epoch 8 acc 0.2254 loss 3.9800\n",
            "Single epoch took : 554.948573589325 seconds\n",
            "\n",
            "Epoch 9 acc 0.2254 loss 3.8343 batch_no. 0  \n",
            "Epoch 9 acc 0.2287 loss 3.8636 batch_no. 200  \n",
            "Epoch 9 acc 0.2320 loss 3.8447 batch_no. 400  \n",
            "Epoch 9 acc 0.2352 loss 3.8195 batch_no. 600  \n",
            "Epoch 9 acc 0.2384 loss 3.8044 batch_no. 800  \n",
            "Epoch 9 acc 0.2416 loss 3.7901\n",
            "Single epoch took : 554.2505114078522 seconds\n",
            "\n",
            "Epoch 10 acc 0.2416 loss 4.2214 batch_no. 0  \n",
            "Epoch 10 acc 0.2447 loss 3.6952 batch_no. 200  \n",
            "Epoch 10 acc 0.2477 loss 3.6750 batch_no. 400  \n",
            "Epoch 10 acc 0.2508 loss 3.6566 batch_no. 600  \n",
            "Epoch 10 acc 0.2538 loss 3.6453 batch_no. 800  \n",
            "Saving checkpoint for epoch 10 at checkpoints/ckpt-2\n",
            "Epoch 10 acc 0.2568 loss 3.6344\n",
            "Single epoch took : 555.5219919681549 seconds\n",
            "\n",
            "Epoch 11 acc 0.2568 loss 3.6291 batch_no. 0  \n",
            "Epoch 11 acc 0.2598 loss 3.5595 batch_no. 200  \n",
            "Epoch 11 acc 0.2627 loss 3.5459 batch_no. 400  \n",
            "Epoch 11 acc 0.2655 loss 3.5335 batch_no. 600  \n",
            "Epoch 11 acc 0.2684 loss 3.5252 batch_no. 800  \n",
            "Epoch 11 acc 0.2712 loss 3.5177\n",
            "Single epoch took : 554.9436807632446 seconds\n",
            "\n",
            "Epoch 12 acc 0.2712 loss 3.7190 batch_no. 0  \n",
            "Epoch 12 acc 0.2739 loss 3.4795 batch_no. 200  \n",
            "Epoch 12 acc 0.2766 loss 3.4703 batch_no. 400  \n",
            "Epoch 12 acc 0.2793 loss 3.4568 batch_no. 600  \n",
            "Epoch 12 acc 0.2820 loss 3.4472 batch_no. 800  \n",
            "Epoch 12 acc 0.2846 loss 3.4408\n",
            "Single epoch took : 555.3909442424774 seconds\n",
            "\n",
            "Epoch 13 acc 0.2846 loss 3.5503 batch_no. 0  \n",
            "Epoch 13 acc 0.2872 loss 3.3933 batch_no. 200  \n",
            "Epoch 13 acc 0.2897 loss 3.3811 batch_no. 400  \n",
            "Epoch 13 acc 0.2923 loss 3.3705 batch_no. 600  \n",
            "Epoch 13 acc 0.2947 loss 3.3605 batch_no. 800  \n",
            "Epoch 13 acc 0.2972 loss 3.3521\n",
            "Single epoch took : 555.1821441650391 seconds\n",
            "\n",
            "Epoch 14 acc 0.2972 loss 3.3780 batch_no. 0  \n",
            "Epoch 14 acc 0.2996 loss 3.2949 batch_no. 200  \n",
            "Epoch 14 acc 0.3020 loss 3.2853 batch_no. 400  \n",
            "Epoch 14 acc 0.3044 loss 3.2738 batch_no. 600  \n",
            "Epoch 14 acc 0.3067 loss 3.2631 batch_no. 800  \n",
            "Epoch 14 acc 0.3091 loss 3.2569\n",
            "Single epoch took : 554.7674286365509 seconds\n",
            "\n",
            "Epoch 15 acc 0.3091 loss 3.4092 batch_no. 0  \n",
            "Epoch 15 acc 0.3114 loss 3.1997 batch_no. 200  \n",
            "Epoch 15 acc 0.3136 loss 3.1868 batch_no. 400  \n",
            "Epoch 15 acc 0.3159 loss 3.1744 batch_no. 600  \n",
            "Epoch 15 acc 0.3181 loss 3.1659 batch_no. 800  \n",
            "Saving checkpoint for epoch 15 at checkpoints/ckpt-3\n",
            "Epoch 15 acc 0.3203 loss 3.1596\n",
            "Single epoch took : 555.5053541660309 seconds\n",
            "\n",
            "Epoch 16 acc 0.3203 loss 3.0973 batch_no. 0  \n",
            "Epoch 16 acc 0.3225 loss 3.1028 batch_no. 200  \n",
            "Epoch 16 acc 0.3247 loss 3.0924 batch_no. 400  \n",
            "Epoch 16 acc 0.3268 loss 3.0810 batch_no. 600  \n",
            "Epoch 16 acc 0.3289 loss 3.0743 batch_no. 800  \n",
            "Epoch 16 acc 0.3310 loss 3.0681\n",
            "Single epoch took : 555.0396926403046 seconds\n",
            "\n",
            "Epoch 17 acc 0.3310 loss 3.3609 batch_no. 0  \n",
            "Epoch 17 acc 0.3331 loss 3.0070 batch_no. 200  \n",
            "Epoch 17 acc 0.3351 loss 2.9984 batch_no. 400  \n",
            "Epoch 17 acc 0.3372 loss 2.9894 batch_no. 600  \n",
            "Epoch 17 acc 0.3392 loss 2.9848 batch_no. 800  \n",
            "Epoch 17 acc 0.3412 loss 2.9800\n",
            "Single epoch took : 554.598806142807 seconds\n",
            "\n",
            "Epoch 18 acc 0.3412 loss 3.1941 batch_no. 0  \n",
            "Epoch 18 acc 0.3432 loss 2.9139 batch_no. 200  \n",
            "Epoch 18 acc 0.3451 loss 2.9193 batch_no. 400  \n",
            "Epoch 18 acc 0.3471 loss 2.9088 batch_no. 600  \n",
            "Epoch 18 acc 0.3490 loss 2.9057 batch_no. 800  \n",
            "Epoch 18 acc 0.3509 loss 2.9010\n",
            "Single epoch took : 554.7599744796753 seconds\n",
            "\n",
            "Epoch 19 acc 0.3509 loss 3.1342 batch_no. 0  \n",
            "Epoch 19 acc 0.3528 loss 2.8477 batch_no. 200  \n",
            "Epoch 19 acc 0.3546 loss 2.8451 batch_no. 400  \n",
            "Epoch 19 acc 0.3565 loss 2.8359 batch_no. 600  \n",
            "Epoch 19 acc 0.3583 loss 2.8323 batch_no. 800  \n",
            "Epoch 19 acc 0.3601 loss 2.8283\n",
            "Single epoch took : 554.7092080116272 seconds\n",
            "\n",
            "Epoch 20 acc 0.3601 loss 2.9432 batch_no. 0  \n",
            "Epoch 20 acc 0.3619 loss 2.7748 batch_no. 200  \n",
            "Epoch 20 acc 0.3637 loss 2.7761 batch_no. 400  \n",
            "Epoch 20 acc 0.3655 loss 2.7657 batch_no. 600  \n",
            "Epoch 20 acc 0.3672 loss 2.7641 batch_no. 800  \n",
            "Saving checkpoint for epoch 20 at checkpoints/ckpt-4\n",
            "Epoch 20 acc 0.3689 loss 2.7602\n",
            "Single epoch took : 555.6550779342651 seconds\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uPDgjlhX9Et"
      },
      "source": [
        "def mod_eval(text):\n",
        "    text = tkniser_en.texts_to_sequences([text])\n",
        "    text = tf.keras.preprocessing.sequence.pad_sequences(text, maxlen=len_enc, \n",
        "                                                                   padding='post', truncating='post')\n",
        "\n",
        "    ip_encdr = tf.expand_dims(text[0], 0)\n",
        "\n",
        "    ip_dcrdr = [tkniser_hi.word_index['<sos>']]\n",
        "    output = tf.expand_dims(ip_dcrdr, 0)\n",
        "    \n",
        "    for i in range(len_dcdr):\n",
        "        msk_encdr_padding, msk_comb, msk_dcdr_padding = make_msk(ip_encdr, output)\n",
        "\n",
        "        predictions, wts_attn = transformer(\n",
        "            ip_encdr, \n",
        "            output,\n",
        "            False,\n",
        "            msk_encdr_padding,\n",
        "            msk_comb,\n",
        "            msk_dcdr_padding\n",
        "        )\n",
        "\n",
        "        predictions = predictions[: ,-1:, :]\n",
        "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "\n",
        "        if predicted_id == tkniser_hi.word_index['<eos>']:\n",
        "            return tf.squeeze(output, axis=0), wts_attn\n",
        "\n",
        "        output = tf.concat([output, predicted_id], axis=-1)\n",
        "\n",
        "    return tf.squeeze(output, axis=0), wts_attn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5e4FAaoNbpbE"
      },
      "source": [
        "def translate(txtin_english):\n",
        "    txtin_hindi = mod_eval(text=txtin_english)[0].numpy()\n",
        "    txtin_hindi = np.expand_dims(txtin_hindi[1:], 0)  \n",
        "    return tkniser_hi.sequences_to_texts(txtin_hindi)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIghAKjAMBfU"
      },
      "source": [
        "# Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "xA09eMYNbtn4",
        "outputId": "86241336-9a5c-4e09-c93a-a809abb9bc07"
      },
      "source": [
        "translate(\"My name is Shambhavi.\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'मेरा नाम तमिल नाम है।'"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "qzB1GJcpbw63",
        "outputId": "919e1d1d-53f5-4700-d83c-062a266906f7"
      },
      "source": [
        "translate(\"It's a beautiful place.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'यह एक छोटी इमारतें है खास तौर पर बहुत बड़ा है।'"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08YYdQC6FySA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f2f6fbd0-bbd6-4480-ad23-0a6e72da5e4e"
      },
      "source": [
        "translate(\"She gave him a watch.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'वो एक महान उपहार थे ।'"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "DtGLYmoldkbl",
        "outputId": "a8d73b0e-298e-47c1-8de9-71dc06bca233"
      },
      "source": [
        "translate(\"thank you\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'धन्यवाद धन्यवाद धन्यवाद धन्यवाद धन्यवाद धन्यवाद धन्यवाद'"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "zpNw6mVydxXU",
        "outputId": "1a855aa2-d93d-403c-803c-30b0380d1b57"
      },
      "source": [
        "translate(\"Where do we go?\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'जहाँ हम कहाँ कहाँ करते हैं'"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "GBU8wFL1d3yQ",
        "outputId": "cad69a15-b42c-4124-e752-ce5062c56c44"
      },
      "source": [
        "translate(\"I am going home.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'मैं घर घर में बाहर जाती हूँ'"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "MCju8nFgeHoE",
        "outputId": "c0dbdee1-f8ae-4605-b8fa-5e92c81c59a5"
      },
      "source": [
        "translate(\"I have to go.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'मुझे हमेशा देखना होगा'"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "anZaGnLBeOOj",
        "outputId": "2c02e907-a2c8-4902-fd04-94643c4b3a04"
      },
      "source": [
        "translate(\"That is a cat.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'कि ये एक रंग है जिसमें कि एक रंग'"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Op55wfIueaQN",
        "outputId": "2289d70b-1ae1-455d-8ec2-463e5d848e14"
      },
      "source": [
        "translate(\"That politics , in retrospect , was rooted in a false ideology.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'इतिहास में आतंकवाद का एक राजनैतिक मुद्दा है'"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2171yeVeqlA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}